# -*- coding: utf-8 -*-
"""Pythonfiles.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14XKYflrUrzC20zX6fCzv5MCccpf_AKHp
"""

import tensorflow as tf

# Step 1: Create a random tensor of shape (4, 6)
tensor = tf.random.uniform(shape=(4, 6), minval=0, maxval=10, dtype=tf.float32)

# Step 2: Find its rank and shape
rank = tf.rank(tensor)
shape = tensor.shape

# Print rank and shape before reshaping
print(f"Original Tensor:\n{tensor.numpy()}")
print(f"Rank: {rank.numpy()}, Shape: {shape}")

# Step 3: Reshape the tensor into (2, 3, 4)
reshaped_tensor = tf.reshape(tensor, (2, 3, 4))

# Transpose the reshaped tensor to (3, 2, 4)
transposed_tensor = tf.transpose(reshaped_tensor, perm=[1, 0, 2])

# Print rank and shape after reshaping/transposing
print(f"\nReshaped Tensor:\n{reshaped_tensor.numpy()}")
print(f"Shape after reshaping: {reshaped_tensor.shape}")

print(f"\nTransposed Tensor:\n{transposed_tensor.numpy()}")
print(f"Shape after transposing: {transposed_tensor.shape}")

# Step 4: Broadcasting a smaller tensor (1, 4) to match the larger tensor
small_tensor = tf.random.uniform(shape=(1, 4), minval=0, maxval=10, dtype=tf.float32)

# Broadcasting the smaller tensor to match the shape of reshaped_tensor
broadcasted_tensor = small_tensor + reshaped_tensor  # TensorFlow automatically expands dimensions

# Print the result of broadcasting
print(f"\nSmaller Tensor:\n{small_tensor.numpy()}")
print(f"\nBroadcasted Addition Result:\n{broadcasted_tensor.numpy()}")

# Step 5: Explanation of broadcasting
broadcasting_explanation = """
In TensorFlow, broadcasting allows smaller tensors to be expanded automatically to match
the shape of larger tensors in element-wise operations.

Example: A tensor of shape (1, 4) can be broadcasted to match a tensor of shape (2, 3, 4).
Here, the single row of the (1, 4) tensor is duplicated across the first two dimensions
to allow addition to the (2, 3, 4) tensor without explicit reshaping.
"""

print(broadcasting_explanation)

import numpy as np
import matplotlib.pyplot as plt

# Step 1: Define true values (y_true) and model predictions (y_pred)
# For MSE (Regression)
y_true_mse = np.array([3.0, 5.0, 2.5, 7.0, 6.5])
y_pred_mse_1 = np.array([2.8, 5.2, 2.7, 6.8, 6.2])  # Slight deviation
y_pred_mse_2 = np.array([3.5, 5.5, 3.0, 7.5, 7.0])  # More deviation

# For CCE (Classification - One-hot encoded)
y_true_cce = np.array([[0, 1, 0], [1, 0, 0]])  # True labels
y_pred_cce_1 = np.array([[0.1, 0.8, 0.1], [0.7, 0.2, 0.1]])  # Close to correct
y_pred_cce_2 = np.array([[0.3, 0.6, 0.1], [0.6, 0.3, 0.1]])  # More deviation

# Step 2: Compute Mean Squared Error (MSE) Loss
def mean_squared_error(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

mse_loss_1 = mean_squared_error(y_true_mse, y_pred_mse_1)
mse_loss_2 = mean_squared_error(y_true_mse, y_pred_mse_2)

# Step 3: Compute Categorical Cross-Entropy (CCE) Loss
def categorical_cross_entropy(y_true, y_pred):
    y_pred = np.clip(y_pred, 1e-9, 1.0)  # Avoid log(0)
    return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]

cce_loss_1 = categorical_cross_entropy(y_true_cce, y_pred_cce_1)
cce_loss_2 = categorical_cross_entropy(y_true_cce, y_pred_cce_2)

# Print loss values
print(f"MSE Loss (Prediction 1): {mse_loss_1:.4f}")
print(f"MSE Loss (Prediction 2 - Modified): {mse_loss_2:.4f}")
print(f"Categorical Cross-Entropy Loss (Prediction 1): {cce_loss_1:.4f}")
print(f"Categorical Cross-Entropy Loss (Prediction 2 - Modified): {cce_loss_2:.4f}")

# Step 4: Plot loss function values using Matplotlib
loss_labels = ["MSE (Pred 1)", "MSE (Pred 2)", "CCE (Pred 1)", "CCE (Pred 2)"]
loss_values = [mse_loss_1, mse_loss_2, cce_loss_1, cce_loss_2]

plt.figure(figsize=(8, 5))
plt.bar(loss_labels, loss_values, color=['blue', 'cyan', 'red', 'orange'])
plt.xlabel("Loss Types")
plt.ylabel("Loss Value")
plt.title("Comparison of MSE and Categorical Cross-Entropy Loss")
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# Step 1: Load the MNIST dataset
mnist = fetch_openml('mnist_784', version=1)
X, y = mnist.data.astype(np.float32), mnist.target.astype(np.int32)

# Normalize pixel values to range [0,1]
X /= 255.0

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features for better training stability
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 2: Train models using Adam and SGD
def train_model(optimizer):
    model = MLPClassifier(hidden_layer_sizes=(128,), activation='relu', solver=optimizer,
                          max_iter=5, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return accuracy_score(y_test, y_pred)

# Train models with Adam and SGD optimizers
accuracy_adam = train_model('adam')
accuracy_sgd = train_model('sgd')

# Step 3: Compare accuracy values
optimizers = ['Adam', 'SGD']
accuracies = [accuracy_adam, accuracy_sgd]

# Step 4: Plot accuracy comparison
plt.figure(figsize=(6, 4))
plt.bar(optimizers, accuracies, color=['blue', 'orange'])
plt.ylim([0.85, 1.0])  # Set a reasonable accuracy range
plt.xlabel("Optimizer")
plt.ylabel("Validation Accuracy")
plt.title("Adam vs. SGD Optimizer Performance on MNIST")
plt.show()

import tensorflow as tf
import datetime
import numpy as np
import matplotlib.pyplot as plt

# Step 1: Load and preprocess the MNIST dataset
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize the data to [0,1] range
x_train, x_test = x_train / 255.0, x_test / 255.0

# Step 2: Define a simple neural network model
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Step 3: Set up TensorBoard logging
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

# Train the model with TensorBoard logging enabled
history = model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test), callbacks=[tensorboard_callback])

# Step 4: Launch TensorBoard
print("\nTo view TensorBoard logs, run the following command in the terminal:")
print(f"tensorboard --logdir={log_dir}")

# Plot Training & Validation Accuracy and Loss
epochs = range(1, 6)
plt.figure(figsize=(12, 5))

# Accuracy Plot
plt.subplot(1, 2, 1)
plt.plot(epochs, history.history['accuracy'], label='Training Accuracy')
plt.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Training vs Validation Accuracy")
plt.legend()

# Loss Plot
plt.subplot(1, 2, 2)
plt.plot(epochs, history.history['loss'], label='Training Loss')
plt.plot(epochs, history.history['val_loss'], label='Validation Loss')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training vs Validation Loss")
plt.legend()

plt.show()